# app.py
from rag_pipeline import load_vectorstore
from langchain_community.llms import Ollama
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains.retrieval import create_retrieval_chain
from langchain_core.prompts import ChatPromptTemplate

def main():
    print("Äang táº£i cÆ¡ sá»Ÿ dá»¯ liá»‡u Ä‘Ã£ lÆ°u...")
    vectordb = load_vectorstore("persistent_chroma_db")
    retriever = vectordb.as_retriever(search_kwargs={"k": 5})

    # MÃ´ hÃ¬nh LLM tá»« Ollama (cháº¡y local)
    llm = Ollama(model="ontocord/vinallama")

    # Prompt cho quÃ¡ trÃ¬nh há»i â€“ Ä‘Ã¡p
    prompt = ChatPromptTemplate.from_template("""
    HÃ£y tráº£ lá»i cÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng báº±ng tiáº¿ng Viá»‡t dá»±a trÃªn ngá»¯ cáº£nh dÆ°á»›i Ä‘Ã¢y.
    Náº¿u khÃ´ng cÃ³ thÃ´ng tin phÃ¹ há»£p, hÃ£y tráº£ lá»i "TÃ´i khÃ´ng cháº¯c cháº¯n vá» Ä‘iá»u Ä‘Ã³."

    Ngá»¯ cáº£nh:
    {context}

    CÃ¢u há»i:
    {input}
    """)

    # Táº¡o chuá»—i xá»­ lÃ½ tÃ i liá»‡u trÆ°á»›c khi Ä‘Æ°a vÃ o LLM
    question_answer_chain = create_stuff_documents_chain(llm, prompt)

    # Káº¿t há»£p chuá»—i há»i Ä‘Ã¡p vÃ  bá»™ truy xuáº¥t dá»¯ liá»‡u (RAG)
    rag_chain = create_retrieval_chain(retriever, question_answer_chain)

    print("Há»‡ thá»‘ng sáºµn sÃ ng! GÃµ 'exit' Ä‘á»ƒ thoÃ¡t.\n")
    while True:
        query = input("CÃ¢u há»i: ")
        if query.lower() in ["exit", "quit"]:
            print("ğŸ‘‹ Táº¡m biá»‡t!")
            break

        result = rag_chain.invoke({"input": query})
        print(f"ğŸ’¡ Tráº£ lá»i: {result['answer']}\n")

if __name__ == "__main__":
    main()
